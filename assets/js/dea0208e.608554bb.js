"use strict";(self.webpackChunkapp_docs=self.webpackChunkapp_docs||[]).push([[3041],{3905:(e,t,n)=>{n.d(t,{Zo:()=>p,kt:()=>h});var o=n(67294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,o)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,o,a=function(e,t){if(null==e)return{};var n,o,a={},i=Object.keys(e);for(o=0;o<i.length;o++)n=i[o],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(o=0;o<i.length;o++)n=i[o],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var l=o.createContext({}),d=function(e){var t=o.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},p=function(e){var t=d(e.components);return o.createElement(l.Provider,{value:t},e.children)},c={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},u=o.forwardRef((function(e,t){var n=e.components,a=e.mdxType,i=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),u=d(n),h=a,m=u["".concat(l,".").concat(h)]||u[h]||c[h]||i;return n?o.createElement(m,r(r({ref:t},p),{},{components:n})):o.createElement(m,r({ref:t},p))}));function h(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var i=n.length,r=new Array(i);r[0]=u;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:a,r[1]=s;for(var d=2;d<i;d++)r[d]=n[d];return o.createElement.apply(null,r)}return o.createElement.apply(null,n)}u.displayName="MDXCreateElement"},36098:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>c,frontMatter:()=>i,metadata:()=>s,toc:()=>d});var o=n(87462),a=(n(67294),n(3905));const i={sidebar_position:11,description:"How to create and integrate a Hugging Face Inference Endpoint"},r="How to create and integrate a Hugging Face Inference Endpoint",s={unversionedId:"tutorials/ai/huggingface",id:"tutorials/ai/huggingface",title:"How to create and integrate a Hugging Face Inference Endpoint",description:"How to create and integrate a Hugging Face Inference Endpoint",source:"@site/docs/tutorials/ai/11-huggingface.md",sourceDirName:"tutorials/ai",slug:"/tutorials/ai/huggingface",permalink:"/docs/tutorials/ai/huggingface",draft:!1,editUrl:"https://github.com/nevermined-io/app_docs/tree/main/docs/tutorials/ai/11-huggingface.md",tags:[],version:"current",sidebarPosition:11,frontMatter:{sidebar_position:11,description:"How to create and integrate a Hugging Face Inference Endpoint"},sidebar:"tutorialSidebar",previous:{title:"Monetizing a ChatGPT plugin using Nevermined",permalink:"/docs/tutorials/ai/chatgpt-plugin"},next:{title:"Nevermined Public Environments",permalink:"/docs/environments/"}},l={},d=[{value:"Requirements",id:"requirements",level:2},{value:"Deploy a Hugging Face model",id:"deploy-a-hugging-face-model",level:2},{value:"Implement and deploy your own AI Model",id:"implement-and-deploy-your-own-ai-model",level:2},{value:"Create a new Model repository",id:"create-a-new-model-repository",level:3},{value:"Implementing a simple example with Haystack",id:"implementing-a-simple-example-with-haystack",level:3},{value:"Expose the custom Model as Inference Endpoint",id:"expose-the-custom-model-as-inference-endpoint",level:3},{value:"Testing the Inference Endpoint",id:"testing-the-inference-endpoint",level:3},{value:"Publish the Inference Endpoint in Nevermined",id:"publish-the-inference-endpoint-in-nevermined",level:2},{value:"Before you register your Service",id:"before-you-register-your-service",level:3},{value:"Registering the AI Model",id:"registering-the-ai-model",level:3},{value:"Access to the details of the Service",id:"access-to-the-details-of-the-service",level:3},{value:"Consuming your AI Model",id:"consuming-your-ai-model",level:3},{value:"Examples",id:"examples",level:4}],p={toc:d};function c(e){let{components:t,...i}=e;return(0,a.kt)("wrapper",(0,o.Z)({},p,i,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"how-to-create-and-integrate-a-hugging-face-inference-endpoint"},"How to create and integrate a Hugging Face Inference Endpoint"),(0,a.kt)("p",null,"Hugging Face is one of the most, if not the most important Open Source Communities, in the scope of Machine Learning and AI technologies."),(0,a.kt)("p",null,"It is not only a huge hub of models, datasets, and transformers, but also an environment where users can deploy, test, and productize AI models or pipelines."),(0,a.kt)("p",null,"Hugging Face allows users to implement and deploy models, transformers, pipelines, etc, in two different ways:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"https://huggingface.co/docs/hub/spaces"},"Spaces")," Based on Github repos, users can implement their models here and deploy it as an app for free (it's also possible to pay for additional resources). This kind of deployment is for demo/fast development purposes, so it is not ready for production."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"https://huggingface.co/docs/inference-endpoints/index"},"Inference Endpoints"),". With Inference Endpoints a user or organization is able to deploy a model into production in a cloud provider in a completely transparent way. Hugging Face will take care of deploying the necessary containers, securitization, auto scaling, etc")),(0,a.kt)("p",null,"In this tutorial we will show you how to deploy your models and pipelines as Inference Endpoints, and how to publish them in Nevermined App, so you can safely share and monetize your AI model."),(0,a.kt)("h2",{id:"requirements"},"Requirements"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"A Hugging Face ",(0,a.kt)("a",{parentName:"li",href:"https://huggingface.co/join"},"Account")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"https://huggingface.co/settings/billing"},"Payment Method added"),". Inference Endpoints are not for free. We will show you how to keep the cost pretty low so you can test it without spending too much money."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"https://huggingface.co/docs/hub/security-tokens#how-to-manage-user-access-tokens"},"Generate a token")," for your account with Read permissions.")),(0,a.kt)("h2",{id:"deploy-a-hugging-face-model"},"Deploy a Hugging Face model"),(0,a.kt)("p",null,"You can deploy any model available in Hugging Face Hub in a really straightforward way. This ",(0,a.kt)("a",{parentName:"p",href:"https://huggingface.co/docs/inference-endpoints/guides/create_endpoint"},"document")," from Hugging Face's documentation shows how to deploy a model from a repository."),(0,a.kt)("p",null,"In this tutorial we want to show you how to implement and deploy a more complex scenario, where you are not deploying an already built model, but a more elaborated process."),(0,a.kt)("h2",{id:"implement-and-deploy-your-own-ai-model"},"Implement and deploy your own AI Model"),(0,a.kt)("p",null,"Through the next sections you will learn how to implement a custom AI model or process, deploy it as Inference Endpoint and publish it in Nevermined App."),(0,a.kt)("h3",{id:"create-a-new-model-repository"},"Create a new Model repository"),(0,a.kt)("p",null,"In order to implement a custom model you have to create a new model repository. You can work locally in your implementation and use git to create and synchronize in Hugging Face, as it's explained in ",(0,a.kt)("a",{parentName:"p",href:"https://huggingface.co/docs/inference-endpoints/guides/custom_handler"},"Hugging Face Documentation"),". In this document you will find different examples of custom models."),(0,a.kt)("p",null,"Also you can manually create this model repository and add/edit the files using the UI. For the sake of simplicity we are doing this in this tutorial."),(0,a.kt)("p",null,"To create this repository you just need to use the button ",(0,a.kt)("em",{parentName:"p"},"New")," you can find in the main page of your Huggin Face profile (or your organization profile) "),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"Create a new Model option",src:n(8835).Z,width:"941",height:"300"})),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"Create a new Model formulary",src:n(75059).Z,width:"1096",height:"811"})),(0,a.kt)("p",null,"Once the model repository is created you can use the ",(0,a.kt)("em",{parentName:"p"},"Add")," button to create and commit new files in the repository. Also you have the possibility to edit the files as many times as you need. "),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"Create a new Model files",src:n(36364).Z,width:"1143",height:"519"}),"\n",(0,a.kt)("img",{alt:"Create a new Model add files",src:n(89138).Z,width:"1052",height:"230"})),(0,a.kt)("p",null,"You will typically need a handler.py and a requirements.txt file."),(0,a.kt)("p",null,"Of course creating and editing the files directly in the repo is not the best way to implement your custom model because there is no way to test it before deploying the model as an Inference Point. "),(0,a.kt)("h3",{id:"implementing-a-simple-example-with-haystack"},"Implementing a simple example with Haystack"),(0,a.kt)("p",null,'As we mentioned, in the Hugging Face documentation you can find multiple examples about how to expose your own model using custom Inference Endpoints. For our example we will take a different approach, to show you that there are more possibilities than "just" use a model.'),(0,a.kt)("p",null,"In this case we are going to use ",(0,a.kt)("a",{parentName:"p",href:"https://haystack.deepset.ai"},"Haystack")," to build a InMemory store where we will index a document related with DeSci DAOs, and using ",(0,a.kt)("a",{parentName:"p",href:"https://huggingface.co/deepset/roberta-base-squad2-distilled"},"roberta-base-squad2-distilled")," we can implement a Q&A service using the documents stored."),(0,a.kt)("p",null,"The fist step is to add the needed dependencies using the requirements.txt file:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-txt"},"farm-haystack==1.19.0\nfarm-haystack[inference]==1.19.0\nvalidators==0.21.1\n")),(0,a.kt)("p",null,"Next we need to implement an EndpointHandler class in a handler.py file. This class will contain two methods, ",(0,a.kt)("strong",{parentName:"p"},"init"),", where we will place all the code that will be executed when the endpoint is starting (so it will executed only once), and ",(0,a.kt)("strong",{parentName:"p"},"call"),", that handles the execution calls."),(0,a.kt)("p",null,"In the ",(0,a.kt)("strong",{parentName:"p"},"init")," method we will place all the code to initialize Haystack and to read and index the file with the DeSci DAOs information. In the ",(0,a.kt)("strong",{parentName:"p"},"call")," method we read the payload of the call to get the question, and we call the Haystack pipeline to get an answer."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-py"},'import os\nfrom haystack.utils import fetch_archive_from_http, clean_wiki_text, convert_files_to_docs\nfrom haystack.schema import Answer\nfrom haystack.document_stores import InMemoryDocumentStore\nfrom haystack.pipelines import ExtractiveQAPipeline\nfrom haystack.nodes import FARMReader, TfidfRetriever\nimport logging\nimport json\n\nos.environ[\'TOKENIZERS_PARALLELISM\'] ="false"\n\n#Haystack Components\ndef start_haystack():\n    document_store = InMemoryDocumentStore()\n    load_and_write_data(document_store)\n    retriever = TfidfRetriever(document_store=document_store)\n    reader = FARMReader(model_name_or_path="deepset/roberta-base-squad2-distilled", use_gpu=True)\n    pipeline = ExtractiveQAPipeline(reader, retriever)\n    return pipeline\n\ndef load_and_write_data(document_store):\n    \n    # Get the absolute path of the script\n    script_path = os.path.realpath(__file__)\n    # Get the script directory\n    script_dir = os.path.dirname(script_path)\n    doc_dir = script_dir + "/dao_data"\n    print("Loading data ...")\n\n    docs = convert_files_to_docs(dir_path=doc_dir, clean_func=clean_wiki_text, split_paragraphs=True)\n    document_store.write_documents(docs)\n\n\nclass EndpointHandler():\n    def __init__(self, path=""):\n        # load the optimized model     \n        self.pipeline = start_haystack()\n\n    def __call__(self, data):\n        \n        inputs = data.pop("inputs", None)\n        question = inputs.pop("question", None)\n        if question is not None:\n            prediction = self.pipeline.run(query=question, params={"Retriever": {"top_k": 10}, "Reader": {"top_k": 5}})\n        else:\n            return {}\n        \n        # postprocess the prediction\n        response = { "answer": prediction[\'answers\'][0].answer}\n        return json.dumps(response)\n')),(0,a.kt)("p",null,"And that's it. We can now deploy this process as an Inference Endpoint. But remember, in a real scenario you should implement and test your model/process in your local environment to check that everything works correctly."),(0,a.kt)("h3",{id:"expose-the-custom-model-as-inference-endpoint"},"Expose the custom Model as Inference Endpoint"),(0,a.kt)("p",null,"The next step is to publish your model as an Inference Endpoint. Remember, you need to add a Payment Method to be able to use this Hugging Face's capability."),(0,a.kt)("p",null,"YOu can access to Inference Endpoints ",(0,a.kt)("a",{parentName:"p",href:"https://ui.endpoints.huggingface.co/"},"here"),". In the main screen you can see the endpoints you have already deployed, and a ",(0,a.kt)("em",{parentName:"p"},"New endpoint")," button to create a new one"),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"Inference Endpoints",src:n(53863).Z,width:"1235",height:"410"})),(0,a.kt)("p",null,"In the New Endpoint formulary, you have to introduce the name of the repository where you have implemented your model. Also you need to choose the Cloud Provider (you can leave the default one)."),(0,a.kt)("p",null,"You can pick a small Instance type, instead of the medium one, to keep the cost low (You can see the estimated cost per hour at the bottom of the screen). Take into account that this also implies that the execution time will be longer."),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"Create new Inference Endpoint",src:n(66337).Z,width:"910",height:"957"})),(0,a.kt)("p",null,"In the ",(0,a.kt)("em",{parentName:"p"},"Advanced Configuration")," section you can find some useful configuration. For instance you can enable the ",(0,a.kt)("em",{parentName:"p"},"Automatic Scale-to-Zero")," option so the endpoint will be effectively paused where there is no activity (You can pause and resume your endpoint manually as well). The time the endpoint is paused won't be billed."),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"Advanced Configuration",src:n(66940).Z,width:"910",height:"730"})),(0,a.kt)("p",null,"Once you finish to set all the configuration, click in the ",(0,a.kt)("em",{parentName:"p"},"Create Endpoint")," button, and the Inference Endpoint will be initialized.\n",(0,a.kt)("img",{alt:"Inference Endpoint Initializing",src:n(21904).Z,width:"1226",height:"582"})),(0,a.kt)("p",null,"After a few minutes you will see your Inference Endpoint running and the screen will show you the URL where it is located. "),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"Inference Endpoint Created",src:n(1872).Z,width:"1191",height:"569"})),(0,a.kt)("p",null,"In the ",(0,a.kt)("em",{parentName:"p"},"Settings")," menu you can change the configuration of the Inference Endpoint, or update it to the latest version in your repository."),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"Inference Endpoint Settings",src:n(73145).Z,width:"1224",height:"779"})),(0,a.kt)("h3",{id:"testing-the-inference-endpoint"},"Testing the Inference Endpoint"),(0,a.kt)("p",null,"To test the Inference Endpoint you just need the URL and your Read Token."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},'export HF_TOKEN="api_orgweRt....srbCQpQ"\n\ncurl https://clmdl67ebofnkfxk.eu-west-1.aws.endpoints.huggingface.cloud \\\n-X POST \\\n-d \'{"inputs":{"question":"What is VitaDAO?"}}\' \\\n-H "Authorization: Bearer $HF_TOKEN" \\\n-H "Content-Type: application/json"\n')),(0,a.kt)("p",null,"If the execution of the endpoint takes around (or over) a minute, we will recommend you to change the settings of the Instance Type, for instance from ",(0,a.kt)("em",{parentName:"p"},"small")," to ",(0,a.kt)("em",{parentName:"p"},"medium")),(0,a.kt)("p",null,"If you have deployed the Inference endpoint under an organization, any user who belongs to the organization will be able to access to the endpoint using their own Read Tokens."),(0,a.kt)("h2",{id:"publish-the-inference-endpoint-in-nevermined"},"Publish the Inference Endpoint in Nevermined"),(0,a.kt)("p",null,"Once you have implemented and deployed and protected your own AI model or pipeline in HuggingFace, you can share it with the Community in a safe way, and even monetize them, if you want, using a Nevermined Smart Subscription."),(0,a.kt)("p",null,"In order to test and learn how you can use Nevermined App, we provide a test deployment that uses Arbitrum Goerli testnet, where you can try the different features provided by Nevermined."),(0,a.kt)("p",null,"You can access to this test version of Nevermined App ",(0,a.kt)("a",{parentName:"p",href:"https://goerli.nevermined.app/en"},"here")),(0,a.kt)("h3",{id:"before-you-register-your-service"},"Before you register your Service"),(0,a.kt)("p",null,"We recommend you to take a look to the different ",(0,a.kt)("a",{parentName:"p",href:"../../getting-started/"},"guides and tutorials we have about Nevermined App")),(0,a.kt)("p",null,"Before starting using Nevermined you will need to install and config Metamask in your browser. ",(0,a.kt)("a",{parentName:"p",href:"../first-steps/metamask/"},"See the instructions here"),", ",(0,a.kt)("a",{parentName:"p",href:"../first-steps/metamask-networks#arbitrum-goerli-testnet/"},"here"),", and ",(0,a.kt)("a",{parentName:"p",href:"../first-steps/metamask-tokens/"},"here"),", "),(0,a.kt)("p",null,"Once you have Metamask correctly configured, the next step is to create a brand new ",(0,a.kt)("a",{parentName:"p",href:"../../getting-started/smart-subscriptions"},"Smart Subscription")),(0,a.kt)("p",null,"You will register your AI Service associated with this Subscription you are about to create. The process to create a new Subscription is pretty straightforward, but ",(0,a.kt)("a",{parentName:"p",href:"../first-steps/create-subscription"},"here")," you can find some help to guide you."),(0,a.kt)("h3",{id:"registering-the-ai-model"},"Registering the AI Model"),(0,a.kt)("p",null,"So now that you have all set up and you have created a Smart Subscription, you can create a Web Service Asset to register your AI Model in Nevermined App."),(0,a.kt)("p",null,"You can find a complete guide to register your service ",(0,a.kt)("a",{parentName:"p",href:"../first-steps/register-webservice/"},"here")),(0,a.kt)("p",null,"In the ",(0,a.kt)("em",{parentName:"p"},"details")," step you need to add the endpoint. As the Inference Endpoint does not generate any OpenAPI document, we need to do it manually. "),(0,a.kt)("p",null,"Add the Inference Endpoint URL as a ",(0,a.kt)("em",{parentName:"p"},"HTTP POST")," Protected Method. "),(0,a.kt)("p",null,"Select ",(0,a.kt)("em",{parentName:"p"},"Bearer Token")," and add a Hugging Face Token with Read Permissions. This token will be sent and stored encrypted, so no one will be able to access it."),(0,a.kt)("p",null,"Make sure you provide enough information about your service in the ",(0,a.kt)("em",{parentName:"p"},"Description")," and ",(0,a.kt)("em",{parentName:"p"},"Integration")," fields to allow the user to understand the purpose of your service and how they can use it."),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"Register service in Nevermined",src:n(9086).Z,width:"1087",height:"734"})),(0,a.kt)("h3",{id:"access-to-the-details-of-the-service"},"Access to the details of the Service"),(0,a.kt)("p",null,' When the process is finished, you will be able to access the details of your new Service Asset (you can also access anytime using the "MyAssets" menu on the App).\nIn the Service details you can access the description of the endpoints.'),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"Service details in Nevermined",src:n(93588).Z,width:"1217",height:"748"})),(0,a.kt)("h3",{id:"consuming-your-ai-model"},"Consuming your AI Model"),(0,a.kt)("p",null,"Every user that have purchased your Subscription will be able to use your AI Model through Nevermined. In this ",(0,a.kt)("a",{parentName:"p",href:"../advanced/webservice-integration/"},"guide")," you can find how users can integrate your service."),(0,a.kt)("h4",{id:"examples"},"Examples"),(0,a.kt)("p",null,"Use the service through Nevermined Proxy URL is really straightforward, you need to use the Proxy URL instead of the actual URL of your service, adding the specific endpoint you want to call and the parameters defined in that endpoint, and indicate and the Authorization Header with the JWT."),(0,a.kt)("p",null,"For instance:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},'export NVM_TOKEN="eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..EW-BsszuYJLLuBylm6VPvw.zlGJQcCRjjG_m....srbCQpQ"\n\ncurl -H "Authorization: $NVM_TOKEN" -X POST "https://3dqf53c6wn5sd5crq1n0bzh92cr09kaevh16io0pbefn57kbcj.proxy.goerli.nevermined.app"  \\\n--header \'content-type: application/json\' \\\n--data \'{"inputs":{"question":"What is VitaDAO?"}}\'\n')))}c.isMDXComponent=!0},8835:(e,t,n)=>{n.d(t,{Z:()=>o});const o=n.p+"assets/images/11-01-create-model-a77898bc83dc8c246841a3fb1d827960.png"},75059:(e,t,n)=>{n.d(t,{Z:()=>o});const o=n.p+"assets/images/11-02-create-form-85857d390339f4065c4e088469db4621.png"},36364:(e,t,n)=>{n.d(t,{Z:()=>o});const o=n.p+"assets/images/11-03-Files-534b42fb523eb99f075acd6b273e0941.png"},89138:(e,t,n)=>{n.d(t,{Z:()=>o});const o=n.p+"assets/images/11-04-add-files-ae0b67c5532418fc15f3b82cfadede2b.png"},53863:(e,t,n)=>{n.d(t,{Z:()=>o});const o=n.p+"assets/images/11-05-endpoints-334930175eed4cd4819ef5416f5cb467.png"},66337:(e,t,n)=>{n.d(t,{Z:()=>o});const o=n.p+"assets/images/11-06-new-endpoint-form-3e6caea1014d0f82250f985d1f4d0f06.png"},66940:(e,t,n)=>{n.d(t,{Z:()=>o});const o=n.p+"assets/images/11-07-advanced-config-228dc1abd38fd509b114cbe33ef37a8c.png"},21904:(e,t,n)=>{n.d(t,{Z:()=>o});const o=n.p+"assets/images/11-08-initializing-ca70c090ecea0035f42eb3801132d43c.png"},1872:(e,t,n)=>{n.d(t,{Z:()=>o});const o=n.p+"assets/images/11-09-endpoint-created-5d3756404984323b407779e1bf39ec82.png"},73145:(e,t,n)=>{n.d(t,{Z:()=>o});const o=n.p+"assets/images/11-10-endpoint-settings-0440252e8be6ef4cd4ae30ff8ccb5893.png"},9086:(e,t,n)=>{n.d(t,{Z:()=>o});const o=n.p+"assets/images/11-11-creating-service-nvm-bb823bf62c2948dac647551d8f48840a.png"},93588:(e,t,n)=>{n.d(t,{Z:()=>o});const o=n.p+"assets/images/11-12-nvm-service-details-903df8ea90cbd9866d321cbdfd467021.png"}}]);